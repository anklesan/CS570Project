<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS/ECE 570 Project</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 80px;
            padding-left: 100px;
            padding-right: 100px;
            background-color: rgb(247, 252, 252); /* Cream color */
        }

        h1, h2, h3 {
            color: #000000;
        }

        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 10px;
        }

        p {
            text-align: justify;
        }

        section {
            margin-bottom: 20px;
            padding-left: 100px;
            padding-right: 100px;
            padding-bottom: 20px;
            padding-top: 20px;
            background-color: rgb(187, 217, 227);
            border-radius: 10px;
        }

        .container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1px;
            padding-left: 30px;
            padding-right: 30px;
        }

        .paragraph {
            text-align: center;
        }

        table, th, td {
            border: 1px solid black;
            border-collapse: collapse;
            padding: 10px;
        }

        button {
            font-size:20px;
            background-color: rgb(187, 217, 227);
            padding: 10px;
            border-radius: 10px;
        }
    </style>
</head>

<body>
    <h1><center>CS/ECE570 Project Report</center></h1>
    <h2><center>Heterogeneous Computing in an AI Context</center></h2>
    <h3>
        <p><center>Amber Kahklen & Ninad Anklesaria</center></p>
        <p><center>Winter 2024</center></p>
    </h3>

    <section><p>If you would like to view the project proposal, mid-report, or presentation slides please use the links below:</p>
    <a href="proposal.html" title="Project Proposal"><button type="button">Project Proposal</button></a>
    <a href="midReport.html" title="Project Mid-Report"><button type="button">Project Mid-Report</button></a>
    <a href="Heterogeneous_Computing_in_an_AI_Context_1.pdf" title="Presentation Slides"><button type="button">Presentation Slides</button></a>
    <p></p> 
    </section>

    <section>
        <h3><p><b>0 &nbsp; Overview</b></p></h3>
        <ul>
            <a href="#intro"><li>Introduction</li></a>
            <a href="#back"><li>Background</li></a>
            <ul>
                <a href="#hc"><li>Heterogeneous Computing</li></a>
                <a href="#ai"><li>Artificial Intelligence</li></a>
            </ul>
            <a href="#IANDF"><li>Infrastructure and Frameworks</li></a>
            <a href="#HCforDL"><li>Heterogeneous Computing for Deep Learning</li></a>
            <a href="#AIforHC"><li>AI for Heterogeneous Computing</li></a>
            <a href="#results"><li>Results</li></a>
            <a href="#con"><li>Conclusion and Future Work</li></a>
            <a href="#ref"><li>References</li></a>
        </ul>
    </section>

    <section>
        <p id="intro"><h3><p><b>1 &nbsp; Introduction</b></p></h3></p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp;The advancement of heterogeneous computing has increased the range of options for artificial intelligence 
            (AI), especially deep learning (DL) which often requires large amounts of computing power. This project sought to gather information 
            on the impacts heterogeneous computing and artificial intelligence have had on each other as well as the challenges, benefits, and limitations. 
            More specifically, the goal was to gather information on the many different infrastructure and frameworks used and how they can be used for AI, 
            Machine Learning (ML), or DL. Alongside that, information regarding how AI could, in turn, be used to optimize the use of the given architecture 
            by allowing the AI to assist in the scheduling or design of the system.  This topic was chosen due to the importance of heterogeneous 
            computing alongside an increase in applications of AI with heterogeneous computing. The survey was done to ensure that it included information 
            from both academic research and industry applications to gain a well-rounded understanding from differing perspectives. Overall, heterogeneous computing 
            remains an important concept in computer architecture, one that is being rapidly adopted to be used with artificial intelligence.</p>
    

    <p id="back"><h3><b>2 &nbsp; Background</b></p></h3></p>
    <p id="hc"><b>2.1 &nbsp;What is Heterogeneous Computing?</b></p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous computing is a computing architecture that incorporates multiple types of processors or cores within a single system,
         each specialized for different computational tasks. This approach leverages the strengths of various processing units—such as CPUs, GPUs, DSPs, and FPGAs—to optimize performance, 
         energy efficiency, and computational speed. By allocating tasks to the most suitable processor type, heterogeneous computing systems can handle a wide range of applications more effectively than homogeneous 
         systems.
    </p>

    <center><img src="ECE570_IMG5.png" alt="heterogeneous computing example" style="width:50%;height:50%;"></center>
    <p><center>Figure 1: Overview of Heterogeneous Computing.</center></p>

    <p>
        &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous computing for Artificial Intelligence became more prominent in the late 2000's early 2010's with the release of NVIDIA's 
        CUDA (Compute Unified Device Architecture) in 2006. According to NVIDIA [4], they developed CUDA to make an impact on the computing performance through the use of 
        the GPU and describe CUDA as a parallel computing platform and programming model. CUDA has been used in many different areas since its release, creating a big 
        increase in the use and implementation of heterogeneous computing for various AI implementations. NVIDIA [6] talks about the many different uses of CUDA including how 
        it was not only widely adopted by research but it was also used to develop many common applications such as Adobe, Ansys, Mathworks, and Wolfram Mathematica.
    </p>

    <p id="ai"><b>2.2 &nbsp; Why is it relevant to Artificial intelligence?</b></p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous computing is relevant for AI due to its ability to enhance performance, improve energy efficiency, and provide flexibility for diverse AI workloads. 
        By leveraging various types of processors—such as CPUs for general tasks, GPUs for parallel processing, 
        and specialized accelerators for specific AI functions—heterogeneous computing systems can efficiently handle the intensive computational demands of training and deploying AI models. 
        This approach enables faster processing, reduces power consumption, and supports the scalability and innovation needed for the complex and varied applications of AI technology.</p>
        <p>
        </p>

    <p id="IANDF"><h3><b>3 &nbsp; Infrastructure and Frameworks</b></h3></p>
    <u><p>Key Architectures and Frameworks</p></u>
    <p><ul>
        <li><b>HSA </b>(Heterogeneous System Architecture) 
            <ul>
                <li>Enables efficient hardware acceleration by integrating central processing units (CPUs) and graphics processing units (GPUs)
                    for seamless computation.</li>
            </ul>
        </li>
        <li><b>CUDA (Compute Unified Device Architecture)</b> 
            <ul>
                <li>NVIDIA's platform for general-purpose graphics processing unit (GPGPU) which was pivotal in AI and deep learning.</li>
            </ul>
        </li>
        <li><b>OpenCL (Open Computing Language):</b> Open standard for cross-platform programming across CPUs, GPUs, and more, ensuring flexibiility and broad applicability.</li>
        <li><b>TPU Architecture (Tensor Processing Unit):</b> Google's custom-designed processors optimized for TensorFlow operations, significantly accelerating deep learning computations. </li>
        <li><b>ROCm (Radeon Open Compute):</b> AMD's open source GPU computing framework.</li>
        <li><b>Intel oneAPI:</b> A unified programming model by Intel to streamline development across CPUs, GPUs, field programmable gate arrays (FPGAs), and AI accelerators.</li>
    </ul></p>
    <b>3.1 &nbsp;HSA (Heterogeneous System Architecture)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous System Architecture (HSA) integrates both central processing units (CPUs) and graphics processing units (GPUs) 
        for shared memory and tasks. HSA is led by the HSA foundation which includes both AMD and ARM. HSA reduces latency between compute devices, simplifies 
        programming, and enchances the execution performance of programming languages and models like CUDA and OpenCL. HSA enables direct GPU floating point 
        calculations without separate scheduling.
    </p>

    <center><img src="ECE570_IMG6.png" alt="HSA" style="width:70%;height:70%;"></center>
    <p><center>Figure 2: Overview of Heterogeneous System Architecture.</center></p>


    <b>3.2 &nbsp; Programming Models for Heterogeneous Computing</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Shown in the list below are the common programming models for heterogeneous computing going by level 
        of abstraction.
        <ul>
            <li>CUDA: is a NVIDIA proprietary</li>
            <li>OpenCL: Open standard and is functionally portable across multi-cores</li>
            <li>OpenACC: High-level and pragma-based</li>
            <li>Different libraries, programming models, and DSLs for different domains</li>
        </ul>
        It is possible to use either a mix of programming models such as one or several models for CPUs with OpenMP or one or several models 
        for GPUs instead with CUDA. It is also possible to just use a single unified programming model rather than mixing different ones. The figure 
        below shows the difference in level between various popular programming models.
    </p>

    <center><img src="ECE570_IMG9.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 3: Low versus high level programming models.</center></p>

    <b>3.2.1 &nbsp;CUDA (Compute Unified Device Architecture)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; CUDA is NVIDIA's programming platform for GPGPUs that is designed for heterogeneous computing. CUDA is 
        designed so that some of the functions are run on the CPU while others are run on the GPU allowing for acceleration in the process. 
        Programs are typically written in C or C++ with annotations for the GPU executution. A typical CUDA program flow can be seen in the 
        figure below.
    </p>

    <center><img src="ECE570_IMG7.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 4: Typical CUDA program flow.</center></p>

    <center><img src="ECE570_IMG8.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 5: Overview of CUDA uses.</center></p>

    <b>3.2.2 &nbsp;OpenCL (Open Computing Language)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; 

    </p>

    <b>3.3 &nbsp;TPU Architecture (Tensor Processing Unit)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; 

    </p>

    <b>3.4 &nbsp;ROCm (Radeon Open Compute)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; 

    </p>

    <b>3.5 &nbsp;Intel oneAPI</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; 

    </p>

    <p id="HCforDL"><h3><b>4 &nbsp; Heterogeneous Computing for Deep Learning</b></p></h3></p>

            <p> &nbsp; &nbsp; &nbsp; &nbsp; Malita <i>et al.</i> [3] explores the challenges and advancements in hardware acceleration for deep learning. It details the computational components of Deep Neural Networks (DNNs), 
                reviews state-of-the-art hardware solutions including Intel's MIC, Nvidia's GPUs, and Google's TPUs. 
            </p>
            <p>Computational Components of DNNs: They outline the key computational components of Deep Neural Networks (DNNs), including fully connected layers, 
                convolutional layers, pooling layers, and softmax layers. There is significant computational intensity of these components and they underscore 
                the importance of efficient acceleration for optimal performance in deep learning tasks. By identifying and understanding these components, researchers and engineers can develop hardware architectures that specifically target the computational </p>
            <p>State of the Art Hardware Solutions: They have reviewed various state-of-the-art hardware solutions commonly used for accelerating deep learning tasks. There is a discussion of various architectures such as Intel's Many Integrated Core (MIC) processors, Nvidia's Graphics Processing Units (GPUs), 
                and Google's Tensor Processing Units (TPUs). This discussion covers performance characteristics, energy efficiency, and limitations of each architecture in the context of deep learning applications. They conclude that by examining the strengths and weaknesses of these hardware solutions, 
                researchers can make informed decisions when selecting or designing hardware platforms for deep learning tasks.</p>
            <p>Limitations of Specific ASICs like TPU: They delve into the limitations of specific Application-Specific Integrated Circuits (ASICs), focusing on Google's Tensor Processing Units (TPUs). While TPUs offer significant computational power, they lack the flexibility to efficiently 
                support the diverse computational demands of different DNNs. The chapter [3] highlights issues related to flexibility, resource utilization, memory hierarchy, and architectural suitability. By understanding these limitations, researchers can explore alternative approaches or develop 
                strategies to mitigate the challenges associated with specific ASICs like TPUs.</p>
            <p>Conclusion and Future Directions: The authors summarize the key findings of the chapter and offer insights into potential avenues for future research and development. They emphasize that there is a need for innovative architectural designs that can adapt to the evolving landscape of deep learning while 
                optimizing for performance and energy efficiency. The chapter also discusses emerging trends in hardware acceleration for deep learning, such as the integration of specialized hardware units for specific tasks and the exploration of novel memory hierarchy designs. 
            </p>

    <p id="AIforHC"><h3><b>5 &nbsp; Artificial Intelligence for Heterogeneous Computing</b></h3></p>
    <p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp; Memeti <i>et al.</i> [1] bring forward an approach to heterogeneous computing that utilizes AI heuristic search
            techniques in combination with the use of machine learning to optimize the use of the heterogeneous system. The authors reference the 
            Enumeration and Measurements (EnuM) technique in comparison to their own when evaluating their method which they call AI heuristics with machine 
            learning (AML). EnuM is also known as brute-force search and it evaluates every option available before making a decision based on those results. The 
            authors aim to determine a near optimal system configuration using their proposed method. Their proposed method uses the heuristic search as a guide 
            through the parameter space and uses simulated annealing (SA) to conduct the parameter space exploration. The outline of this approach can be seen below in
            Figure 1:
        </p>
        <center><img src="ECE570_IMG1.png" alt="Memeti et al. approach" style="width:50%;height:50%;"></center>
        <p><center>Figure 1: Memeti <i>et al.</i> [1] AML design.</center></p>
        <p>
            &nbsp; &nbsp; &nbsp; &nbsp; The authors then use decision tree regression, 
            a supervised machine learning model, to evaluate the system configuration. The results of their experiment shows that AML is more than 1300 times faster
            than EnuM. Another noted difference between the two is AML is able to achieve a similar energy efficiency to EnuM after only evaluating around 7% of the
            possible configurations while EnuM needs to evaluate all of the possible configurations to achieve that energy efficiency.
        </p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp; Greathouse <i>et al.</i> [2] take a different approach and use machine learning (ML) to predict an applications performance and
            power on a range of heterogeneous systems. The authors begin the process by creating the dataset to be used by the model by running various applications on
            different combinations of hardware and storing that data. Using a fully connected neural network with a linear input layer and sigmoid functions for the hidden
            and output layers the kernels are trained and clustered as can be seen in Figure 2. 
        </p>

        <center><img src="ECE570_IMG3.png" alt="Greathouse et al. approach" style="width:40%;height:40%;"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="ECE570_IMG4.png" alt="Greathouse et al. approach closeup." style="width:40%;height:40%;"></center>
        <div class="container">
            <p class="paragraph">Figure 2: Greathouse <i>et al.</i> [1] clustering setup. <br> Each kernel produces a scaling curve and similar <br> kernels are grouped together into clusters.</p>
            <p class="paragraph">Figure 3: Greathouse <i>et al.</i> [1] kernel example closeup</p>
            
        </div>
        <p>
            &nbsp; &nbsp; &nbsp; &nbsp; Within the clusters the authors set it to pick the kernel with the highest value as the representative
            for that cluster. The model is then used with an application to predict and a cluster is picked based on which cluster the application is closest to and the scaling
            curve designed by the authors using the collected data is used to predict performance or power of the kernel depending on desired configuration. This model is organized into
            the overall model setup seen in Figure 4:
        </p>
        <center><img src="ECE570_IMG2.png" alt="Greathouse et al. approach overview" style="width:40%;height:40%;"></center>
        <p><center>Figure 4: Greathouse <i>et al.</i> [1] overall model setup.</center></p>
        <p>
            &nbsp; &nbsp; &nbsp; &nbsp; It is determined that the use of the designed system is very effective and does not require heavy computational effort. The authors tested the accuracy of their model and found it to have
            only 15% error rate in performance predictions and less than 10% error rate for power. </p>
    
    </p>

    <p id="results"><h3><b>6 &nbsp; Results</b></p></h3></p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp; The survey has yielded various different approaches to this topic that allow for improvement on implementations through the use
            of AI applications. Each approach has had various purposes to attempt to improve upon the efficiency in one way or another. Memeti <i>et al.</i> [1] focused on the 
            optimization of the systems configuration while Greathouse <i>et al.</i> [2] focused on the prediction of performance and energy consumption to reduce time and energy
            costs in industry by reducing the need for physical tests or simulations.
        </p>
    
    <p id="con"><h3><p><b>7 &nbsp; Conclusion and Future Work</b></p></h3></p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; There are many different approaches and implementations regarding heterogeneous computing specifically in an AI context and the expansive number
        of possibilities are evident through the sources included in this survey. Heterogeneous computing can be incredibly useful for AI just as AI can also be extremely helpful
        in the configuration or prediction of heterogeneous systems and their behavior. Throughout the course of the term, our team seeks to continue conduct a thorough survey on 
        heterogeneous computing in an artificial intelligence context. We plan to dive deeper into the various implementations and possibilities pulling in approaches regarding
        the use of heterogeneous computing for AI. We aim to take a look into the different frameworks for heterogeneous computing and how these frameworks are used for AI applications.
        We are also interested in researching more about the most effective algorithms or implementations for the most common heterogeneous implementations.
        Once those topics have been fully researched, we aim to bring together a presentation of the many different implementations of this topic and provide an comprehensive overview. 
        The result of this project should provide good insight into heterogeneous computing in addition to the increasing use of artificial intelligence within the field.
    </p>
    
    <p><u><b>Deliverables</b></u></p>
    <p>Presentation:
    <ul>
    <li><u>Deliverable Due:</u> March 12, 2024</li>
    <li><u>Description: </u> The final presentation will showcase the work done over the course of the term. The 
        presentation will outline the work done by the team and display results collected through the survey.
    </ul>
    </p>
    
    <p>Final Report:
    <ul>
    <li><u>Deliverable Due:</u> March 19, 2024</li>
    <li><u>Description: </u> The final report will go into detail regarding the project description, information collected, 
        and resulting analysis of the collected information. The final report will go into a more detailed view of the overall project and its results.
    </ul>
    </p>
    

    <b><u>Timeline</b></u></p>
        <table>
            <tr>
                <th>Tasks</th>
                <th>Deadline</th>
            </tr>
            <tr>
                <td>Project idea has been decided on and initial research done, project proposal drafted and website created.</td>
                <td>Week 4</td>
            </tr>
            <tr>
                <td>Website and report have been edited and revised. Survey 50% done and mid-report in progress.</td>
                <td>Week 6</td>
            </tr>
            <tr>
                <td>Mid-report completed and website updated with mid-report.</td>
                <td>Week 7</td>
            </tr>
            <tr>
                <td>Survey completed and all data collected. Survey data is ready to be used.</td>
                <td>Week 9</td>
            </tr>
            <tr>
                <td>Presentation finished and being revised/practiced.</td>
                <td>Week 10</td>
            </tr>
            <tr>
                <td>Final Report finished and website updated with final report.</td>
                <td>Final's week - March 19th by 5pm</td>
            </tr>
        </table>
        <p></p> 
    </section>

    <section>
        <p id="ref"><b><u>References</u></b></p>
        <p>
            <ol>
                <li>Memeti, S., Pllana, S. <i>Optimization of heterogeneous systems with AI planning heuristics and machine learning: a performance and energy aware approach.</i> Computing 103, 2943-2966 (2021). https://doi.org/10.1007/s00607-021-01017-6</li>
                <li>J. L. Greathouse and G. H. Loh, <i>Machine Learning for Performance and Power Modeling of Heterogeneous Systems,</i> 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), San Diego, CA, USA, 2018, pp. 1-6, doi: 10.1145/3240765.3243484. </li>
                <li>Mihaela Maliţa, George Vlǎduţ Popescu, & Ştefan, G. M. (2019). <i>Heterogeneous Computing System for Deep Learning.</i> Studies in Computational Intelligence, 287-319. https://doi.org/10.1007/978-3-030-31756-0_10</li>
                <li><i>CUDA FAQ.</i> NVIDIA Developer. (n.d.). https://developer.nvidia.com/cuda-faq </li>
                <li><i>CUDA in Action - Research & Apps.</i> NVIDIA Developer. (n.d.). https://developer.nvidia.com/cuda-action-research-apps</li>
                <li>Dasgupta, S. (2024, January 2). <i>AI drives the software-defined heterogeneous computing era. Energy-Efficient AI Processors and Acceleration.</i> https://www.edgecortix.com/en/blog/ai-drives-the-software-defined-heterogeneous-computing-era#:~:text=Heterogeneous%20Computing%20in%20an%20AI%20Context&text=Heterogeneity%20can%20involve%20different%20instruction,%2C%20cost%2C%20and%20greater%20flexibility</li>
                <li><i>Specifications | one</i>API. (n.d.). OneAPI.io. https://www.oneapi.io/spec/</li>
                <li><i>Accelerating Machine Learning with OpenCL.</i> (2022, May 11). The Khronos Group. https://www.khronos.org/events/accelerating-machine-learning-with-opencl</li>
                <li>Liu, X., Ounifi, H.-A., Gherbi, A., Li, W., & Cheriet, M. (2019). A hybrid GPU-FPGA based design methodology for enhancing machine learning applications performance. <i>Journal of Ambient Intelligence and Humanized Computing,</i> 11(6), 2309-2323. https://doi.org/10.1007/s12652-019-01357-4</li>
                <li><i>Heterogeneous System Architecture.</i> (2023, December 29). Wikipedia. https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture#</li>
                <li><i>Deep Learning Software.</i> (n.d.). NVIDIA Developer. https://developer.nvidia.com/deep-learning-software</li>
                <li>Martínez, P. A., Peccerillo, B., Bartolini, S., García, J. M., & Bernabé, G. (2022). Applying Intel's oneAPI to a machine learning case study. <i>Concurrency and Computation: Practice and Experience,</i> 34(13). https://doi.org/10.1002/cpe.6917</li>
            </ol>
        </p>

    </section>
</body>

</html>
