<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS/ECE 570 Project</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 80px;
            padding-left: 100px;
            padding-right: 100px;
            background-color: rgb(247, 252, 252); /* Cream color */
        }

        h1, h2, h3 {
            color: #000000;
        }

        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 10px;
        }

        p {
            text-align: justify;
        }

        section {
            margin-bottom: 20px;
            padding-left: 100px;
            padding-right: 100px;
            padding-bottom: 20px;
            padding-top: 20px;
            background-color: rgb(187, 217, 227);
            border-radius: 10px;
        }

        .container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1px;
            padding-left: 30px;
            padding-right: 30px;
        }

        .paragraph {
            text-align: center;
        }

        table, th, td {
            border: 1px solid black;
            border-collapse: collapse;
            padding: 10px;
        }

        button {
            font-size:20px;
            background-color: rgb(187, 217, 227);
            padding: 10px;
            border-radius: 10px;
        }
    </style>
</head>

<body>
    <h1><center>CS/ECE570 Project Report</center></h1>
    <h2><center>Heterogeneous Computing in an AI Context</center></h2>
    <h3>
        <p><center>Amber Kahklen & Ninad Anklesaria</center></p>
        <p><center>Winter 2024</center></p>
    </h3>

    <section><p>If you would like to view the project proposal, mid-report, or presentation slides please use the links below:</p>
    <a href="proposal.html" title="Project Proposal"><button type="button">Project Proposal</button></a>
    <a href="midReport.html" title="Project Mid-Report"><button type="button">Project Mid-Report</button></a>
    <a href="Heterogeneous_Computing_in_an_AI_Context_1.pdf" title="Presentation Slides"><button type="button">Presentation Slides</button></a>
    <p></p> 
    </section>

    <section>
        <h3><p><b>0 &nbsp; Overview</b></p></h3>
        <ul>
            <a href="#intro"><li>Introduction</li></a>
            <a href="#back"><li>Background</li></a>
            <ul>
                <a href="#hc"><li>Heterogeneous Computing</li></a>
                <a href="#ai"><li>Artificial Intelligence</li></a>
            </ul>
            <a href="#IANDF"><li>Infrastructure and Frameworks</li></a>
            <a href="#HCforDL"><li>Heterogeneous Computing for Deep Learning</li></a>
            <a href="#AIforHC"><li>AI for Heterogeneous Computing</li></a>
            <a href="#results"><li>Results</li></a>
            <a href="#con"><li>Conclusion and Future Work</li></a>
            <a href="#ref"><li>References</li></a>
        </ul>
    </section>

    <section>
        <p id="intro"><h3><p><b>1 &nbsp; Introduction</b></p></h3></p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp;The advancement of heterogeneous computing has increased the range of options for artificial intelligence 
            (AI), especially deep learning (DL) which often requires large amounts of computing power. This project sought to gather information 
            on the impacts heterogeneous computing and artificial intelligence have had on each other as well as the challenges, benefits, and limitations. 
            More specifically, the goal was to gather information on the many different infrastructure and frameworks used and how they can be used for AI, 
            Machine Learning (ML), or DL. Alongside that, information regarding how AI could, in turn, be used to optimize the use of the given architecture 
            by allowing the AI to assist in the scheduling or design of the system.  This topic was chosen due to the importance of heterogeneous 
            computing alongside an increase in applications of AI with heterogeneous computing. The survey was done to ensure that it included information 
            from both academic research and industry applications to gain a well-rounded understanding from differing perspectives. Overall, heterogeneous computing 
            remains an important concept in computer architecture, one that is being rapidly adopted for use with artificial intelligence.</p>
    

    <p id="back"><h3><b>2 &nbsp; Background</b></p></h3></p>
    <p id="hc"><b>2.1 &nbsp;What is Heterogeneous Computing?</b></p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous computing is a computing architecture that incorporates multiple types of processors or cores within a single system,
         each specialized for different computational tasks. This approach leverages the strengths of various processing units—such as CPUs, GPUs, DSPs, and FPGAs—to optimize performance, 
         energy efficiency, and computational speed. By allocating tasks to the most suitable processor type, heterogeneous computing systems can handle a wide range of applications more effectively than homogeneous 
         systems.
    </p>

    <center><img src="ECE570_IMG5.png" alt="heterogeneous computing example" style="width:50%;height:50%;"></center>
    <p><center>Figure 1: Overview of Heterogeneous Computing by Mohamed [13]</center></p>

    <p>
        &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous computing for Artificial Intelligence became more prominent in the late 2000's early 2010's with the release of NVIDIA's 
        CUDA (Compute Unified Device Architecture) in 2006. According to NVIDIA [4], they developed CUDA to make an impact on the computing performance through the use of 
        the GPU and describe CUDA as a parallel computing platform and programming model. CUDA has been used in many different areas since its release, creating a big 
        increase in the use and implementation of heterogeneous computing for various AI implementations. NVIDIA [6] talks about the many different uses of CUDA including how 
        it was not only widely adopted by research but it was also used to develop many common applications such as Adobe, Ansys, Mathworks, and Wolfram Mathematica.
    </p>

    <p id="ai"><b>2.2 &nbsp; Why is it relevant to Artificial intelligence?</b></p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous computing is relevant for AI due to its ability to enhance performance, improve energy efficiency, and provide flexibility for diverse AI workloads. 
        By leveraging various types of processors—such as CPUs for general tasks, GPUs for parallel processing, 
        and specialized accelerators for specific AI functions—heterogeneous computing systems can efficiently handle the intensive computational demands of training and deploying AI models. 
        This approach enables faster processing, reduces power consumption, and supports the scalability and innovation needed for the complex and varied applications of AI technology.</p>
        <p>
        </p>

    <p id="IANDF"><h3><b>3 &nbsp; Infrastructure and Frameworks</b></h3></p>
    <u><p>Key Architectures and Frameworks</p></u>
    <p><ul>
        <li><b>HSA </b>(Heterogeneous System Architecture) 
            <ul>
                <li>Enables efficient hardware acceleration by integrating central processing units (CPUs) and graphics processing units (GPUs)
                    for seamless computation.</li>
            </ul>
        </li>
        <li><b>CUDA (Compute Unified Device Architecture)</b> 
            <ul>
                <li>NVIDIA's platform for general-purpose graphics processing unit (GPGPU) which was pivotal in AI and deep learning.</li>
            </ul>
        </li>
        <li><b>OpenCL (Open Computing Language):</b> Open standard for cross-platform programming across CPUs, GPUs, and more, ensuring flexibiility and broad applicability.</li>
        <li><b>TPU Architecture (Tensor Processing Unit):</b> Google's custom-designed processors optimized for TensorFlow operations, significantly accelerating deep learning computations. </li>
        <li><b>ROCm (Radeon Open Compute):</b> AMD's open source GPU computing framework.</li>
        <li><b>Intel oneAPI:</b> A unified programming model by Intel to streamline development across CPUs, GPUs, field programmable gate arrays (FPGAs), and AI accelerators.</li>
    </ul></p>
    <b>3.1 &nbsp;HSA (Heterogeneous System Architecture)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Heterogeneous System Architecture (HSA) integrates both central processing units (CPUs) and graphics processing units (GPUs) 
        for shared memory and tasks. HSA is led by the HSA foundation which includes both AMD and ARM. HSA reduces latency between compute devices, simplifies 
        programming, and enchances the execution performance of programming languages and models like CUDA and OpenCL. HSA enables direct GPU floating point 
        calculations without separate scheduling.
    </p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; 
        The rationale behind HSA is to simplify the task for programmers when offloading calculations to the GPU. Originally driven by AMD, this concept expanded to encompass processing units beyond GPUs, such as DSPs from different manufacturers. In a non-HSA system, depicted on the left, offloading calculations to the GPU involves discrete steps and data transfers between CPU and GPU memory. Conversely, in a HSA system leveraging HSA functionality, illustrated on the right, the process is streamlined with direct access to system memory for both CPU and GPU, reducing data transfer overhead and enhancing efficiency.        
    </p>

    

    <center><img src="ECE570_IMG6.png" alt="HSA" style="width:70%;height:70%;"></center>
    <p><center>Figure 2: Overview of Heterogeneous System Architecture. [10]</center></p>


    <b>3.2 &nbsp; Programming Models for Heterogeneous Computing</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Shown in the list below are the common programming models for heterogeneous computing going by level 
        of abstraction.
        <ul>
            <li>CUDA: is a NVIDIA proprietary</li>
            <li>OpenCL: Open standard and is functionally portable across multi-cores</li>
            <li>OpenACC: High-level and pragma-based</li>
            <li>Different libraries, programming models, and DSLs for different domains</li>
        </ul>
        It is possible to use either a mix of programming models such as one or several models for CPUs with OpenMP or one or several models 
        for GPUs instead with CUDA. It is also possible to just use a single unified programming model rather than mixing different ones. The figure 
        below shows the difference in level between various popular programming models.
    </p>

    <center><img src="ECE570_IMG9.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 3: Low versus high level programming models.</center></p>

    <b>3.2.1 &nbsp;CUDA (Compute Unified Device Architecture)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; CUDA is NVIDIA's programming platform for GPGPUs that is designed for heterogeneous computing. CUDA is 
        designed so that some of the functions are run on the CPU while others are run on the GPU allowing for acceleration in the process. 
        Programs are typically written in C or C++ with annotations for the GPU executution. A typical CUDA program flow can be seen in the 
        figure below.
    </p>

    <center><img src="ECE570_IMG7.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 4: Typical CUDA program flow.</center></p>

    <center><img src="ECE570_IMG8.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 5: Overview of CUDA Deep learning platform. [11]</center></p>

    <b>3.2.2 &nbsp;OpenCL (Open Computing Language)</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; OpenCL is a cross-platform framework that enables the execution of code across different hardware platforms. 
        It supports heterogeneous systems by working with various computing devices including CPUs, GPUs, DSPs, and FPGAs. Another feature of 
        OpenCL is the write once, run anywhere approach that means that programs written in OpenCL can run efficiently across multiple hardware architectures 
        which support the OpenCL standard. Below you can see an overview of OpenCL.
    </p>

    <center><img src="ECE570_IMG10.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 6: Overview of OpenCL.[8]</center></p>

    <center><img src="ECE570_IMG11.png" alt="HSA" style="width:50%;height:50%;"></center>
    <p><center>Figure 7: Overview of how OpenCL is integrated with common machine learning frameworks/compilers.[8]</center></p>

    <b>3.3 &nbsp;Intel oneAPI</b>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; Intel oneAPI is a unified programming model designed to streamline development across Intel's diverse hardware 
        (CPUs, GPUs, FPGAs, AI accelerators). The core language used is DPC++ which is based on ISO C++ and SYCL standards. SYCL is a cross-platform abstraction layer 
        that allows algorithms to switch between hardware accelerators such as CPUs, GPUs, and GPGAs. DPC++ is aimed at leveraging the parallel computing 
        potential of Intel's hardware.
    </p>
    <p><center><b>DPC++ = C++ + SYCL + community extensions</b></center></p>

    <p> &nbsp; &nbsp; &nbsp; &nbsp; The workload categorization classifies workloads into scalar, vector, matrix, and spatial domains to optimize 
        computing across different Intell processors. Currently, Intel is pitching oneAPI as a viable CUDA alternative and there is a growing trend 
        of migrating to oneAPI for its promise of eliminating vendor lock-in and fostering a diverse computing environment. There are performance gains 
        such that adoptors of oneAPI often report enhanced performance compared to previous implementations.
    </p>

    <center><img src="ECE570_IMG12.png" alt="HSA" style="width:40%;height:40%;"></center>
    <p><center>Figure 8: Overview of Intel oneAPI.[14]</center></p>

    <p> &nbsp; &nbsp; &nbsp; &nbsp; Martínez <i>et al.</i> [12] explored the potential of Intel's oneAPI as a unified programming model for machine learning 
        applications with a focus on revamping the Caffe framework. They performed a comparative performance study for two crucial layers, softmax and convolution, 
        between the original Caffe implementation (with CUDA) and oneAPI implementation on both CPU and GPU platforms. For the Softmax Layer the onAPI version showed a 7x 
        speedup on Intel GPUs compared to the CPU baseline and a 1.75x improvement over NVIDIA A100 GPUs. For the Convolution Layer the GPU comparisons were limited 
        due to compatibility issues (with NVIDIA GPUs) but the oneAPI implementation on Intel CPUs outperformed native Caffe implementations for larger datasets.
        </p>

    <b>3.4 &nbsp;Intel oneAPI (oneDNN) and SYCL Deep Learning example</b>
    <p>In the code given below :-</p>
    <ul>
        <li><strong>Engines and Streams Initialization:</strong> CPU and GPU engines are created for seamless operation across devices, along with a GPU stream for asynchronous execution. This occurs in the code where the engine and stream objects are instantiated, specifying the CPU and GPU engines and creating a GPU stream for asynchronous execution.</li>
        <li><strong>Memory Objects Creation:</strong> Memory objects are established for CPU and GPU memory to prepare for data transfer and processing. Memory objects are created and initialized for both CPU and GPU memory, utilizing functions such as `memory::memory` to allocate memory on the CPU and GPU.</li>
        <li><strong>Data Reordering:</strong> Data is efficiently moved from CPU to GPU memory using the reorder primitive. The reorder primitive is utilized to transfer data from CPU to GPU memory, typically with a function call such as `reorder::execute`.</li>
        <li><strong>Creation of GPU ReLU Primitive:</strong> A ReLU primitive is configured for GPU execution, optimized for GPU acceleration. The ReLU primitive is specifically created for execution on the GPU, often involving function calls such as `eltwise_forward::desc` and `eltwise_forward::primitive_desc`.</li>
        <li><strong>Retrieving Results from GPU Memory:</strong> Results are fetched back to CPU memory after GPU processing. Data is retrieved from GPU memory back to CPU memory using another reorder operation, typically with a function call similar to `reorder::execute`.</li>
        <li><strong>Execution of Primitives:</strong> All primitives, including reorder and ReLU, are executed in sequence for streamlined computation. Primitives are executed sequentially, often using function calls such as `primitive::execute` to perform the computations.</li>
        <li><strong>Validation of Computed Results:</strong> The integrity of results is verified on CPU memory to ensure accuracy. Computed results on CPU memory are validated using comparison operations, typically occurring after the execution of primitives.</li>
    </ul>
    
    
    
    
    <!-- Add your code block here -->
    <pre><code>
            
            #include "example_utils.hpp"
            #include "oneapi/dnnl/dnnl.hpp"
            #include "example_utils.hpp"
            using namespace dnnl;
            using namespace std;
            // [Prologue]
            void fill(memory &mem, const memory::dims &adims) {
                std::vector<float> array(product(adims));
                for (size_t e = 0; e < array.size(); ++e) {
                    array[e] = e % 7 ? 1.0f : -1.0f;
                }
                write_to_dnnl_memory(array.data(), mem);
            }
            int find_negative(memory &mem, const memory::dims &adims) {
                int negs = 0;
                size_t nelems = product(adims);
                std::vector<float> array(nelems);
                read_from_dnnl_memory(array.data(), mem);
                for (size_t e = 0; e < nelems; ++e)
                    negs += array[e] < 0.0f;
                return negs;
            }
            void cross_engine_reorder_tutorial() {
                // [Initialize engine]
                auto cpu_engine = engine(validate_engine_kind(engine::kind::cpu), 0);
                auto gpu_engine = engine(validate_engine_kind(engine::kind::gpu), 0);
                // [Initialize engine]
                // [Initialize stream]
                auto stream_gpu = stream(gpu_engine, stream::flags::in_order);
                // [Initialize stream]
                //  [reorder cpu2gpu]
                const auto tz = memory::dims {2, 16, 1, 1};
                auto m_cpu
                        = memory({{tz}, memory::data_type::f32, memory::format_tag::nchw},
                                cpu_engine);
                auto m_gpu
                        = memory({{tz}, memory::data_type::f32, memory::format_tag::nchw},
                                gpu_engine);
                fill(m_cpu, tz);
                auto r1 = reorder(m_cpu, m_gpu);
                //  [reorder cpu2gpu]
                // [Create a ReLU primitive]
                //  ReLU op descriptor (uses a GPU memory as source memory.
                //  no engine- or implementation-specific information)
                auto relu_d = eltwise_forward::desc(prop_kind::forward,
                        algorithm::eltwise_relu, m_gpu.get_desc(), 0.0f);
                // ReLU primitive descriptor, which corresponds to a particular
                // implementation in the library. Specify engine type for the ReLU
                // primitive. Use a GPU engine here.
                auto relu_pd = eltwise_forward::primitive_desc(relu_d, gpu_engine);
                // ReLU primitive
                auto relu = eltwise_forward(relu_pd);
                // [Create a ReLU primitive]
                //  [reorder gpu2cpu]
                auto r2 = reorder(m_gpu, m_cpu);
                //  [reorder gpu2cpu]
                // [Execute primitives]
                // wrap source data from CPU to GPU
                r1.execute(stream_gpu, m_cpu, m_gpu);
                // Execute ReLU on a GPU stream
                relu.execute(stream_gpu, {{DNNL_ARG_SRC, m_gpu}, {DNNL_ARG_DST, m_gpu}});
                // Get result data from GPU to CPU
                r2.execute(stream_gpu, m_gpu, m_cpu);
                stream_gpu.wait();
                // [Execute primitives]
                // [Check the results]
                if (find_negative(m_cpu, tz) != 0)
                    throw std::logic_error(
                            "Unexpected output, find a negative value after the ReLU "
                            "execution.");
                // [Check the results]
            }
            int main(int argc, char **argv) {
                return handle_example_errors({engine::kind::cpu, engine::kind::gpu},
                        cross_engine_reorder_tutorial);
            } 
    </code></pre>
   
    <p id="HCforDL"><h3><b>4 &nbsp; Heterogeneous Computing for Deep Learning</b></p></h3></p>

        <p> &nbsp; &nbsp; &nbsp; &nbsp; Taking a closer look at heterogeneous computing for deep learning lets take a look at an example load distribution for a 
            Convolutional Neural Network (CNN) which is a popular image recognition/classification deep learning model. Certain tasks would be assigned to different 
            cores such as the CPU would be assigned data management tasks like preprocessing the input data by decoding, normalizing, or augmenting. The CPU could also 
            be assigned the task of orchestration therefore responsible for managing the training loop, preparing the batches or managing the epochs. Alongside those two 
            tasks the CPU may also be assigned to update the parameters such as updating the weights after backpropagation.
        </p>

        <p> &nbsp; &nbsp; &nbsp; &nbsp; If we take a look at the GPU instead it would be assigned tasks such as layer processing, thus speeding up computation of CNN layers 
            such as the convolutions or activations. The GPU may also be assigned to perform the backpropagation thus efficiently computing the gradients and enhancing the learning speed. 
            Moving on to tasks assigned to the FPGA, these tasks might look something like optimizing operations such as executing custom, efficient CNN operations. The FPGA 
            might also be assigned with the inference task, supporting the real-time model deployment with low latency. Lastly, the TPU could be assigned to matrix operations, 
            specializing in large-scale matrix computations. The TPU might also be tasked with boosting the training or inference speeds. This is a good example of an overall load 
            distribution and really showcases the benefits of heterogeneous computing for deep learning.
        </p>

        <p> &nbsp; &nbsp; &nbsp; &nbsp; Malita <i>et al.</i> [3] explores the challenges and advancements in hardware acceleration for deep learning. It details the computational components of Deep Neural Networks (DNNs), 
            reviews state-of-the-art hardware solutions including Intel's MIC, Nvidia's GPUs, and Google's TPUs. 
        </p>

        <p><u>Computational Components of DNNs:</u> They outline the key computational components of Deep Neural Networks (DNNs), including fully connected layers, 
            convolutional layers, pooling layers, and softmax layers. There is significant computational intensity of these components and they underscore 
            the importance of efficient acceleration for optimal performance in deep learning tasks. By identifying and understanding these components, researchers and engineers can develop hardware architectures that specifically target the computational </p>
        <p><u>State of the Art Hardware Solutions:</u> They have reviewed various state-of-the-art hardware solutions commonly used for accelerating deep learning tasks. There is a discussion of various architectures such as Intel's Many Integrated Core (MIC) processors, Nvidia's Graphics Processing Units (GPUs), 
            and Google's Tensor Processing Units (TPUs). This discussion covers performance characteristics, energy efficiency, and limitations of each architecture in the context of deep learning applications. They conclude that by examining the strengths and weaknesses of these hardware solutions, 
            researchers can make informed decisions when selecting or designing hardware platforms for deep learning tasks.</p>
        <p><u>Limitations of Specific ASICs like TPU:</u> They delve into the limitations of specific Application-Specific Integrated Circuits (ASICs), focusing on Google's Tensor Processing Units (TPUs). While TPUs offer significant computational power, they lack the flexibility to efficiently 
            support the diverse computational demands of different DNNs. The chapter [3] highlights issues related to flexibility, resource utilization, memory hierarchy, and architectural suitability. By understanding these limitations, researchers can explore alternative approaches or develop 
            strategies to mitigate the challenges associated with specific ASICs like TPUs.</p>
        <p><u>Conclusion and Future Directions:</u> The authors summarize the key findings of the chapter and offer insights into potential avenues for future research and development. They emphasize that there is a need for innovative architectural designs that can adapt to the evolving landscape of deep learning while 
            optimizing for performance and energy efficiency. The chapter also discusses emerging trends in hardware acceleration for deep learning, such as the integration of specialized hardware units for specific tasks and the exploration of novel memory hierarchy designs. 
        </p>

    <p id="AIforHC"><h3><b>5 &nbsp; Artificial Intelligence for Heterogeneous Computing</b></h3></p>
    <p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp; Memeti <i>et al.</i> [1] bring forward an approach to heterogeneous computing that utilizes AI heuristic search
            techniques in combination with the use of machine learning to optimize the use of the heterogeneous system. The authors reference the 
            Enumeration and Measurements (EnuM) technique in comparison to their own when evaluating their method which they call AI heuristics with machine 
            learning (AML). EnuM is also known as brute-force search and it evaluates every option available before making a decision based on those results. The 
            authors aim to determine a near optimal system configuration using their proposed method. Their proposed method uses the heuristic search as a guide 
            through the parameter space and uses simulated annealing (SA) to conduct the parameter space exploration. The outline of this approach can be seen below in
            Figure 1:
        </p>
        <center><img src="ECE570_IMG1.png" alt="Memeti et al. approach" style="width:50%;height:50%;"></center>
        <p><center>Figure 9: Memeti <i>et al.</i> [1] AML design.</center></p>
        <p>
            &nbsp; &nbsp; &nbsp; &nbsp; The authors then use decision tree regression, 
            a supervised machine learning model, to evaluate the system configuration. The results of their experiment shows that AML is more than 1300 times faster
            than EnuM. Another noted difference between the two is AML is able to achieve a similar energy efficiency to EnuM after only evaluating around 7% of the
            possible configurations while EnuM needs to evaluate all of the possible configurations to achieve that energy efficiency.
        </p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp; Greathouse <i>et al.</i> [2] take a different approach and use machine learning (ML) to predict an applications performance and
            power on a range of heterogeneous systems. The authors begin the process by creating the dataset to be used by the model by running various applications on
            different combinations of hardware and storing that data. Using a fully connected neural network with a linear input layer and sigmoid functions for the hidden
            and output layers the kernels are trained and clustered as can be seen in Figure 2. 
        </p>

        <center><img src="ECE570_IMG3.png" alt="Greathouse et al. approach" style="width:40%;height:40%;"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="ECE570_IMG4.png" alt="Greathouse et al. approach closeup." style="width:40%;height:40%;"></center>
        <div class="container">
            <p class="paragraph">Figure 10: Greathouse <i>et al.</i> [1] clustering setup. <br> Each kernel produces a scaling curve and similar <br> kernels are grouped together into clusters.</p>
            <p class="paragraph">Figure 11: Greathouse <i>et al.</i> [1] kernel example closeup</p>
            
        </div>
        <p>
            &nbsp; &nbsp; &nbsp; &nbsp; Within the clusters the authors set it to pick the kernel with the highest value as the representative
            for that cluster. The model is then used with an application to predict and a cluster is picked based on which cluster the application is closest to and the scaling
            curve designed by the authors using the collected data is used to predict performance or power of the kernel depending on desired configuration. This model is organized into
            the overall model setup seen in Figure 4:
        </p>
        <center><img src="ECE570_IMG2.png" alt="Greathouse et al. approach overview" style="width:40%;height:40%;"></center>
        <p><center>Figure 12: Greathouse <i>et al.</i> [1] overall model setup.</center></p>
        <p>
            &nbsp; &nbsp; &nbsp; &nbsp; It is determined that the use of the designed system is very effective and does not require heavy computational effort. The authors tested the accuracy of their model and found it to have
            only 15% error rate in performance predictions and less than 10% error rate for power. </p>
    
    </p>

    <p id="results"><h3><b>6 &nbsp; Results</b></p></h3></p>
        <p> &nbsp; &nbsp; &nbsp; &nbsp; The results of the survey show a number of different infrastructure and frameworks that are used for heterogeneous computing for AI. These The use of
            HSA in combination with different programming models allows for effective and very useful implementations. The survey also demonstrated how effective heterogeneous computing can 
            be for deep learning and allowed for a better understanding of a possible load breakdown using heterogeneous computing.

        </p>
    
        <p> &nbsp; &nbsp; &nbsp; &nbsp; The survey has yielded various different approaches to AI for heterogeneous computing that allow for improvement on implementations through the use
            of AI applications. Each approach has had various purposes to attempt to improve upon the efficiency in one way or another. Memeti <i>et al.</i> [1] focused on the 
            optimization of the systems configuration while Greathouse <i>et al.</i> [2] focused on the prediction of performance and energy consumption to reduce time and energy
            costs in industry by reducing the need for physical tests or simulations.
        </p>
    
    <p id="con"><h3><p><b>7 &nbsp; Conclusion and Future Work</b></p></h3></p>
    <p> &nbsp; &nbsp; &nbsp; &nbsp; There are many different approaches and implementations regarding heterogeneous computing specifically in an AI context and the expansive number
        of possibilities are evident through the sources included in this survey. Heterogeneous computing can be incredibly useful for AI just as AI can also be extremely helpful
        in the configuration or prediction of heterogeneous systems and their behavior. Throughout the course of the term, our team sought out information regarding the relationship 
        between these two subjects. This report looked into not only the various implementations and possibilities but also the infrastructure and frameworks required to successully 
        implement some of those approaches. This brought about a discussion on the most effective of these infrastructure and what might be possible with future development.
        The result of this project provides good insight into heterogeneous computing in addition to the increasing use of artificial intelligence within the field as well as 
        requirements to successfully utilize the full capabilities. If our team were to continue to work on this project we would be interested in expanding our survey to learn more 
        about the use of AI for heterogeneous computing development as it could greatly reduce common costs associated with this development process. Heterogeneous computing has expanded 
        the possibilities for AI that continue to be developed.
    </p>
    
        <p></p> 
    </section>

    <section>
        <p id="ref"><b><u>References</u></b></p>
        <p>
            <ol>
                <li>Memeti, S., Pllana, S. <i>Optimization of heterogeneous systems with AI planning heuristics and machine learning: a performance and energy aware approach.</i> Computing 103, 2943-2966 (2021). https://doi.org/10.1007/s00607-021-01017-6</li>
                <li>J. L. Greathouse and G. H. Loh, <i>Machine Learning for Performance and Power Modeling of Heterogeneous Systems,</i> 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), San Diego, CA, USA, 2018, pp. 1-6, doi: 10.1145/3240765.3243484. </li>
                <li>Mihaela Maliţa, George Vlǎduţ Popescu, & Ştefan, G. M. (2019). <i>Heterogeneous Computing System for Deep Learning.</i> Studies in Computational Intelligence, 287-319. https://doi.org/10.1007/978-3-030-31756-0_10</li>
                <li><i>CUDA FAQ.</i> NVIDIA Developer. (n.d.). https://developer.nvidia.com/cuda-faq </li>
                <li><i>CUDA in Action - Research & Apps.</i> NVIDIA Developer. (n.d.). https://developer.nvidia.com/cuda-action-research-apps</li>
                <li>Dasgupta, S. (2024, January 2). <i>AI drives the software-defined heterogeneous computing era. Energy-Efficient AI Processors and Acceleration.</i> https://www.edgecortix.com/en/blog/ai-drives-the-software-defined-heterogeneous-computing-era#:~:text=Heterogeneous%20Computing%20in%20an%20AI%20Context&text=Heterogeneity%20can%20involve%20
                    different%20instruction,%2C%20cost%2C%20and%20greater%20flexibility</li>
                <li><i>Specifications | one</i>API. (n.d.). OneAPI.io. https://www.oneapi.io/spec/</li>
                <li><i>Accelerating Machine Learning with OpenCL.</i> (2022, May 11). The Khronos Group. https://www.khronos.org/events/accelerating-machine-learning-with-opencl</li>
                <li>Liu, X., Ounifi, H.-A., Gherbi, A., Li, W., & Cheriet, M. (2019). A hybrid GPU-FPGA based design methodology for enhancing machine learning applications performance. <i>Journal of Ambient Intelligence and Humanized Computing,</i> 11(6), 2309-2323. https://doi.org/10.1007/s12652-019-01357-4</li>
                <li><i>Heterogeneous System Architecture.</i> (2023, December 29). Wikipedia. https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture#</li>
                <li><i>Deep Learning Software.</i> (n.d.). NVIDIA Developer. https://developer.nvidia.com/deep-learning-software</li>
                <li>Martínez, P. A., Peccerillo, B., Bartolini, S., García, J. M., & Bernabé, G. (2022). Applying Intel's oneAPI to a machine learning case study. <i>Concurrency and Computation: Practice and Experience,</i> 34(13). https://doi.org/10.1002/cpe.6917</li>
                <li>Mohamed, K.S. (2020). <i>Reconfigurable and Heterogeneous Computing. In: Neuromorphic Computing and Beyond.</i> Springer, Cham. https://doi.org/10.1007/978-3-030-37224-8_9</li>
                <li>Intel, R. M.-A., Intel, Chandan Damannagari. (2024, February 1). <i>oneAPI: A viable alternative to CUDA lock-in. VentureBeat.</i> https://venturebeat.com/programming-development/oneapi-a-viable-alternative-to-cuda-lock-in/</li>
            </ol>
        </p>

    </section>
</body>

</html>
